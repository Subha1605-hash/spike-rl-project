{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.26.2 --upgrade --quiet\n",
        "\n"
      ],
      "metadata": {
        "id": "MCe-lLIbwzer"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy --upgrade --quiet\n"
      ],
      "metadata": {
        "id": "Mg6VsSBz5rv_",
        "outputId": "561ed5c2-5b0f-4c0b-fe0e-ace4a2dd4e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 --force-reinstall --quiet\n",
        "\n"
      ],
      "metadata": {
        "id": "8BOdICt16yUS",
        "outputId": "293cb433-ed5f-4d97-85ed-ec66deb5462b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Install if not done already\n",
        "!pip install snntorch --quiet\n",
        "\n",
        "\n",
        "#  Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import snntorch as snn\n",
        "import snntorch.functional as SF\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#  Environment with new API enabled\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=None)\n",
        "\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "#  Q-table\n",
        "q_table = np.zeros((n_states, n_actions))\n",
        "\n",
        "#  Hyperparameters\n",
        "episodes = 200\n",
        "learning_rate = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.995\n",
        "min_epsilon = 0.01\n",
        "beta = 0.95  # for LIF neurons\n",
        "\n",
        "#  SNN Encoder\n",
        "class StateEncoder(nn.Module):\n",
        "    def __init__(self, n_states, hidden_size):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(n_states, hidden_size)\n",
        "        self.lif = snn.Leaky(beta=beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cur = self.fc(x)\n",
        "        spk, _ = self.lif(cur)\n",
        "        return spk\n",
        "\n",
        "encoder = StateEncoder(n_states=n_states, hidden_size=16)\n",
        "\n",
        "#  STDP-like weight update\n",
        "stdp_weights = torch.rand((n_states, 16), requires_grad=False)\n",
        "\n",
        "def stdp_update(pre_spikes, post_spikes, reward, lr=0.01):\n",
        "    dw = torch.outer(pre_spikes, post_spikes)\n",
        "    return reward * dw * lr\n",
        "\n",
        "#  One-hot encode state\n",
        "def one_hot(state, n_states):\n",
        "    x = torch.zeros(n_states)\n",
        "    x[state] = 1.0\n",
        "    return x\n",
        "\n",
        "#  Training loop with new step/reset API\n",
        "for ep in range(episodes):\n",
        "    state, info = env.reset()  # returns (observation, info)\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state_tensor = one_hot(state, n_states)\n",
        "        encoded_spikes = encoder(state_tensor)\n",
        "\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Q-learning update\n",
        "        q_table[state, action] += learning_rate * (\n",
        "            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        # STDP update\n",
        "        pre = state_tensor.detach()\n",
        "        post = encoded_spikes.detach()\n",
        "        stdp_weights.data += stdp_update(pre, post, reward)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "print(\"\\n Training complete!\")\n",
        "print(f\"Final epsilon: {epsilon:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPE0wW0lwyQs",
        "outputId": "5440e37c-49d1-443d-fd2d-c81a348e37f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training complete!\n",
            "Final epsilon: 0.367\n"
          ]
        }
      ]
    }
  ]
}
